{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyGk-87qnbWE"
      },
      "source": [
        "# bia-bob demo in Colab\n",
        "\n",
        "\n",
        "This notebook is based on [collama](https://github.com/5aharsh/collama). By default, Google Colab sessions run on a CPU. To speed up LLM model execution, you can switch to a T4 GPU (or a more powerful option if you have a paid Colab plan). To do this, navigate to **Runtime > Change runtime type** and select the desired GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1S1YL6EnYBB"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Install necessary packages\n",
        "\n",
        "1. `pciutils` helps Ollama identify available GPU hardware.\n",
        "2. Ollama is installed in the runtime using `curl -fsSL https://ollama.com/install.sh | sh`.\n",
        "3. The `bia-bob` package is also installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlVK9iG4AD5L",
        "outputId": "4e231fca-ec0e-4cc2-e42f-f1ac13346a77"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip install --no-cache-dir --quiet bia-bob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download some test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://github.com/haesleinhuepf/bia-bob/raw/refs/heads/main/demo/blobs.tif"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGEJwjTPoKWH"
      },
      "source": [
        "### Starting Ollama\n",
        "\n",
        "To use Ollama, it must operate as a background service alongside your notebook code. Since Jupyter Notebooks execute cells sequentially, running a persistent service can be challenging. To address this, we'll launch the Ollama service in a separate Python subprocess, allowing other cells to run without interruption.\n",
        "\n",
        "The service starts with the command `ollama serve`.\n",
        "\n",
        "A short `time.sleep(5)` pause ensures the Ollama service is ready before proceeding to download the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh5CBAFxBYAC"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcBLqZfyoHg4"
      },
      "source": [
        "### Pulling Ollama Model\n",
        "\n",
        "Download the LLM model using `ollama pull llama3.2`.\n",
        "\n",
        "For other models check https://ollama.com/library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2ghppmRDFny",
        "outputId": "373a581e-edd3-432b-db7d-d3ca76d51955"
      },
      "outputs": [],
      "source": [
        "!ollama pull llama3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Try bia-bob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb49kH5BuiF1"
      },
      "outputs": [],
      "source": [
        "from bia_bob import bob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNLpYHpEuwjb",
        "outputId": "5ed682e1-b2ec-47da-fe52-57754722f4ce"
      },
      "outputs": [],
      "source": [
        "bob.initialize(endpoint='ollama', model='llama3.2', api_key='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ETlkP2Nu-Bl",
        "outputId": "a1a1fa6c-421e-483d-ff94-4bfb22435638"
      },
      "outputs": [],
      "source": [
        "%bob load blobs.tif and show it"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
