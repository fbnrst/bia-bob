{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bia-bob with Local LLM via Ollama\n",
    "\n",
    "This notebook shows how to:\n",
    "1. Download the `phi3:mini` model using the Ollama CLI.\n",
    "2. Start the Ollama server locally.\n",
    "3. Initialize bia-bob to use the local LLM server.\n",
    "4. Run a simple prompt to generate bio-image analysis code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the model (run once per environment)\n",
    "!ollama pull phi3:mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Start Ollama server in background\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start the ollama serve process\n",
    "ollama_process = subprocess.Popen(['ollama', 'serve'])\n",
    "\n",
    "# Wait for server to be ready\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"Ollama server started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize bia-bob with the local LLM\n",
    "import bia_bob as bob\n",
    "\n",
    "bob.initialize(\n",
    "    system_prompt=\"You are a bio-image analysis assistant.\",\n",
    "    llm_provider=\"ollama\",\n",
    "    llm_url=\"http://localhost:11434/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Run a test prompt\n",
    "response = bob.ask(\"Write Python code to segment nuclei in a fluorescence microscopy image.\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}